---
layout:     page
title:      Publications
permalink:  /publications/
---

C: Conference, W: Workshop, P: Preprint / *: equal contribution, &dagger: equal advising


<a name="/socplnr"></a>
<h2 class="pubt">[C9] Socratic Planner: Inquiry-Based Zero-Shot Planning for Embodied Instruction Following</h2>
<p class="pubd">
    <span class="authors">Suyeon Shin, Sujin jeon<sup>*</sup>, Junghyun Kim<sup>*</sup>, <span class="u">Gi-Cheon Kang</span><sup>*</sup>, Byoung-Tak Zhang</span><br>
    <span class="conf">ICRA 2025</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2404.15190">Paper</a>
    </span>
</p>
<hr>

<a name="/cvln"></a>
<h2 class="pubt">[P1] Continual Vision-and-Language Navigation</h2>
<p class="pubd">
    <span class="authors">Seongjun Jeong, <span class="u">Gi-Cheon Kang</span>, Seongho Choi, Joochan Kim, Byoung-Tak Zhang</span><br>
    <span class="conf">arXiv preprint 2024</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2403.15049">Paper</a>
    </span>
</p>
<hr>


<a name="/cliprt"></a>
<h2 class="pubt">[W3] CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural Language Supervision</h2>
<p class="pubd">
    <span class="authors"><span class="u">Gi-Cheon Kang<sup>*</sup></span>, Junghyun Kim<sup>*</sup>, Kyuhwan Shim, Jun Ki Lee<sup>&dagger;</sup>, Byoung-Tak Zhang<sup>&dagger;</sup></span><br>
    <span class="conf">CoRL 2024 Workshop on Language and Robot Learning</span>
    <span class="links">
        <a target="_blank" href="https://clip-rt.github.io">Project Page</a>
        <a target="_blank" href="https://arxiv.org/abs/2411.00508">Paper</a>
    </span>
</p>
<hr>


<a name="/pga"></a>
<h2 class="pubt">[C8] PGA: Personalizing Grasping Agents with Single Human-Robot Interaction</h2>
<p class="pubd">
    <span class="authors">Junghyun Kim, <span class="u">Gi-Cheon Kang</span><sup>*</sup>, Jaein Kim<sup>*</sup>, Seoyun Yang, Minjoon Jung, Byoung-Tak Zhang</span><br>
    <span class="conf">IROS 2024</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2310.12547">Paper</a>
    </span>
</p>
<hr>

<a name="prograsp"></a>
<h2 class="pubt">[C7] PROGrasp: Pragmatic Human-Robot Communication for Object Grasping</h2>
<p class="pubd">
    <span class="authors"><span class="u">Gi-Cheon Kang</span>, Junghyun Kim, Jaein Kim, Byoung-Tak Zhang</span><br>
    <span class="conf">ICRA 2024 (Oral)</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2309.07759">Paper</a>
        <a target="_blank" href="https://github.com/gicheonkang/prograsp">Code</a>
    </span>
</p>
<hr>

<a name="/gst"></a>
<h2 class="pubt">[C6] The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training</h2>
<p class="pubd">
    <span class="authors"><span class="u">Gi-Cheon Kang</span>, Sungdong Kim<sup>*</sup>, Jin-Hwa Kim<sup>*</sup>, Donghyun Kwak<sup>*</sup>, Byoung-Tak Zhang</span><br>
    <span class="conf">CVPR 2023</span><br>
    <span class="conf">ICML 2022 Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward</span>
    <span class="links">
        <a target="_blank" href="https://gicheonkang.com/projects/gst/">Project Page</a>
        <a target="_blank" href="https://arxiv.org/abs/2205.12502">Paper</a>
        <a target="_blank" href="https://github.com/gicheonkang/gst-visdial">Code</a>
        <a target="_blank" href="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/gicheonkang/gicheonkang.github.io/master/docs/GST-23-slide.pdf">Slides</a>
        <a target="_blank" href="https://youtu.be/SrOzZRyqezs">Video</a>
    </span>
</p>
<hr>

<a name="/gvcci"></a>
<h2 class="pubt">[C5] GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic Manipulation</h2>
<p class="pubd">
    <span class="authors">Junghyun Kim, <span class="u">Gi-Cheon Kang</span><sup>*</sup>, Jaein Kim<sup>*</sup>, Suyeon Shin, Byoung-Tak Zhang</span><br>
    <span class="conf">IROS 2023 (Oral)</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2307.05963">Paper</a>
        <a target="_blank" href="https://github.com/JHKim-snu/GVCCI">Code</a>
    </span>
</p>
<hr>

<a name="/sfa"></a>
<h2 class="pubt">[W2] Improving Robustness to Texture Bias via Shape-focused Augmentation</h2>
<p class="pubd">
    <span class="authors">Sangjun Lee, Inwoo Hwang, <span class="u">Gi-Cheon Kang</span>, Byoung-Tak Zhang</span><br>
    <span class="conf">CVPR 2022 Workshop on Human-centered Intelligent Services: Safety and Trustworthy</span>
    <span class="links">
        <a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/HCIS/papers/Lee_Improving_Robustness_to_Texture_Bias_via_Shape-Focused_Augmentation_CVPRW_2022_paper.pdf">Paper</a>
    </span>
</p>
<hr>

<a name="/sglkt"></a>
<h2 class="pubt">[C4] Reasoning Visual Dialog with Sparse Graph Learning and Knowledge Transfer</h2>
<p class="pubd">
    <span class="authors"><span class="u">Gi-Cheon Kang</span>, Junseok Park, Hwaran Lee, Byoung-Tak Zhang<sup>*</sup>, Jin-Hwa Kim<sup>*</sup></span><br>
    <span class="conf">EMNLP 2021 Findings</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2004.06698">Paper</a>
        <a target="_blank" href="https://github.com/gicheonkang/sglkt-visdial">Code</a>
        <a target="_blank" href="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/gicheonkang/gicheonkang.github.io/master/docs/SGLKT-21-slide.pdf">Slides</a>
    </span>
</p>
<hr>

<a name="/masn"></a>
<h2 class="pubt">[C3] Attend What You Need: Motion-Appearance Synergistic Networks for Video Question Answering</h2>
<p class="pubd">
    <span class="authors">Ahjeong Seo, <span class="u">Gi-Cheon Kang</span>, Joonhan Park, Byoung-Tak Zhang</span><br>
    <span class="conf">ACL 2021</span>
    <span class="links">
        <a target="_blank" href="https://aclanthology.org/2021.acl-long.481">Paper</a>
        <a target="_blank" href="https://github.com/ahjeongseo/MASN-pytorch">Code</a>
    </span>
</p>
<hr>

<a name="/c3"></a>
<h2 class="pubt">[W1] C<sup>3</sup>: Contrastive Learning for Cross-domain Correspondence in Few-shot Image Generation</h2>
<p class="pubd">
    <span class="authors">Hyukgi Lee, <span class="u">Gi-Cheon Kang</span>, Chang-Hoon Jeong, Hanwool Sul, Byoung-Tak Zhang</span><br>
    <span class="conf">NeurIPS 2021 Workshop on Controllable Generative Modeling in Language and Vision</span>
    <span class="links">
        <a target="_blank" href="https://ctrlgenworkshop.github.io/camready/40/CameraReady/NIPS_Workshop_camera_ready.pdf">Paper</a>
    </span>
</p>
<hr>

<a name="/lpart"></a>
<h2 class="pubt">[C2] Label Propagation Adaptive Resonance Theory for Semi-Supervised Continuous Learning</h2>
<p class="pubd">
    <span class="authors">Taehyeong Kim, Injune Hwang, <span class="u">Gi-Cheon Kang</span>, Won-Seok Choi, Hyunseo Kim, Byoung-Tak Zhang</span><br>
    <span class="conf">ICASSP 2020</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2005.02137">Paper</a>
    </span>
</p>
<hr>

<a name="/dan"></a>
<h2 class="pubt">[C1] Dual Attention Networks for Visual Reference Resolution in Visual Dialog</h2>
<p class="pubd">
    <span class="authors"><span class="u">Gi-Cheon Kang</span>, Jaeseo Lim, Byoung-Tak Zhang</span><br>
    <span class="conf">EMNLP 2019</span><br>
    <span class="conf">ICCV 2019 Workshop on Video Turing Test (Spotlight Talk)</span>
    <span class="links">
        <a target="_blank" href="https://www.aclweb.org/anthology/D19-1209">Paper</a>
        <a target="_blank" href="https://github.com/gicheonkang/DAN-VisDial">Code</a>
        <a target="_blank" href="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/gicheonkang/gicheonkang.github.io/master/files/DAN-19-slide.pdf">Slides</a>        
    </span>
</p>
<hr>


