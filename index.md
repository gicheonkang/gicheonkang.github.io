---
layout:     page
title:
permalink:  /
---

<div class="row">
    <div class="col-md-4 col-sm-6 col-12">
        <img src="/img/cover2.jpg" class="profile">
    </div>
    <div class="col-md-8 col-sm-6 col-12" style="margin-bottom: 0;">
        Postdoctoral Fellow<br>
        AI Institute, Seoul National University<br>
        <a target="_blank" href="mailto:chonkang@snu.ac.kr">chonkang at snu dot ac dot kr</a>
    </div>
</div>
<hr>


<a name="/bio"></a>

# Bio

I recently received my PhD in AI from [Seoul National University][1], advised by [Prof. Byoung-Tak Zhang][3]. My research lies at the intersection of robot learning and multimodal AI, where I focus on building generalist robots by leveraging semantic priors from large pretrained models. Specific topics include: 

- **Robotics Foundation Models**: Building vision-language-action (VLA) models for generalist robot policies, with a focus on grounding web-scale knowledge to robotic control ([CLIP-RT][cliprt]).
- **Embodied Reasoning**: Investigating vision-language models (VLMs) for task planning ([Socratic Planner][socraticplanner]) and reasoning about language instructions ([PROGrasp][prograsp], [PGA][pga], [GVCCI][gvcci]).
- **Vision & Language**: Developing interactive VLMs that engage in continuous, grounded communication with humans about images ([GST][gst], [SGL][sglkt], [DAN][dan]) and videos ([MASN][masn]).

My PhD research has been supported by fellowships from [Youlchon Foundation][youlchon] and [IPAI][ipai]. I was fortunate to collaborate with researchers in NAVER AI and SK T-Brain.

Prior to joining PhD, I did my master study in Cognitive Science at [Seoul National University][1]. Studying cognitive science has sparked my interest in AI and interdisciplinary research. I earned my Bachelor's degree in Computer Science from [Ajou University][5].

<p style='text-align:center; color:#9A2617; font-weight:bold'>I'm on the job market! Please let me know if you have any opportunities for me.</p>

---

<a name="/news"></a>

# News

- [April 2025] Happy to announce that our work (<a href="https://clip-rt.github.io">CLIP-RT</a>) is accepted to <a href="https://roboticsconference.org">RSS 2025</a>!
- [April 2025] I'm selected as a member of <a href="https://sites.google.com/view/rsspioneers2025/">RSS Pioneers</a>!
- [Nov 2024] Happy to release a new robotics foundation model, <a href="https://clip-rt.github.io">CLIP-RT</a>!
- [Aug 2024] I'm selected as a recipient of the Youlchon AI Star Fellowship.
- [Jun 2024] <a href="https://arxiv.org/abs/2310.12547">PGA</a> is accepted at <a href="https://iros2024-abudhabi.org">IROS 2024</a>.
- [Apr 2024] A preprint for embodied instruction following (<a href="https://arxiv.org/abs/2404.15190">Socratic Planner</a>) is released.
- [Mar 2024] I wrote my <a target="_blank" href="docs/Research_Statement_GCKANG.pdf">research statement</a> about what I've been studying.  
- [Mar 2024] A new preprint (<a href="https://arxiv.org/abs/2403.15049">Continual Vision-and-Language Navigation</a>) is released.
- [Jan 2024] <a href="https://arxiv.org/abs/2309.07759">PROGrasp</a> is accepted to <a href="https://2024.ieee-icra.org">ICRA 2024</a>!
- [Dec 2023] I attend <a href="https://sites.google.com/g.skku.edu/brainlink2023/">Brainlink 2023</a>.
- [Nov 2023] I'll give a talk at Dept. of Energy Resources Engineering at Seoul National University (Title: "The Evolution of Language Models:
From Basic NLP to ChatGPT and Beyond").
- [Oct 2023] Two preprints (<a href="https://arxiv.org/abs/2309.07759">PROGrasp</a> and <a href="https://arxiv.org/abs/2310.12547">PGA</a>) are released! 
- [Jun 2023] One paper is accepted to <a href="https://ieee-iros.org">IROS 2023</a>!
- [Mar 2023] Happy to announce that <a href="https://arxiv.org/abs/2205.12502">our paper</a> is accepted to <a href="https://cvpr2023.thecvf.com">CVPR 2023</a>!
- [Jun 2022] One paper is accepted to ICML 2022 <a href="[https://sites.google.com/nycu.edu.tw/hcis/home](https://pretraining.github.io)">Pre-training Workshop</a>.
- [May 2022] Thrilled to announce that our <a href="https://arxiv.org/abs/2205.12502">new preprint</a> is released!
- [Apr 2022] One paper is accepted to CVPR 2022 <a href="https://sites.google.com/nycu.edu.tw/hcis/home">HCIS Workshop</a>.
- [Dec 2021] I gave an invited talk at Korea Software Congress.
- [Oct 2021] One paper is accepted to NeurIPS 2021 CtrlGen Workshop.
- [Aug 2021] One paper is accepted to Findings of EMNLP 2021.
- [May 2021] One paper is accepted to ACL 2021.
- [Sep 2020] I'm starting my Ph.D. in this fall.
- [Jun 2020] From July, I'll join <a href="https://aiis.snu.ac.kr">SNU AI Institute</a> (AIIS) as a researcher.
- [Jan 2020] Our paper has been accepted to ICASSP 2020!
- [Dec 2019] From January, I'll be a research intern at <a href="https://www.skt.ai">SK T-Brain</a>!
- [Nov 2019] I gave a spotlight talk at <a href="https://videoturingtest.github.io">Video Turing Test workshop</a>, ICCV 2019.
- [Oct 2019] I gave an invited talk at <a href="https://www.skt.ai">SK Telecom AI Center</a>.
- [Aug 2019] Excited to announce that <a href="https://arxiv.org/abs/1902.09368">our paper</a> has been accepted to <a href="https://www.emnlp-ijcnlp2019.org/">EMNLP 2019</a>.
- [Jun 2019] Our proposed method ranks <b>3rd place</b> in <a href="https://visualdialog.org/challenge/2019">Visual Dialog Challenge 2019</a>!!
- [Aug 2018] We have a paper accepted to ECCV 2018 Workshop on <a href="http://vizwiz.org/workshop/">VizWiz Grand Challenge</a>.

<div id="read-more-button">
    <a nohref>Read more</a>
</div>

<hr>


# Highlights <span style="font-size: 16px; display: inline;">(<a href="/publications">Here</a> are all papers)</span>


<a name="/cliprt"></a>
<h2 class="pubt">CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural Language Supervision</h2>
<p class="pubd">
    <span class="authors"><span class="u">Gi-Cheon Kang<sup>*</sup></span>, Junghyun Kim<sup>*</sup>, Kyuhwan Shim, Jun Ki Lee<sup>&dagger;</sup>, Byoung-Tak Zhang<sup>&dagger;</sup></span><br>
    <span class="conf">RSS 2025</span><br>
    <span class="conf">CoRL 2024 Workshop on Language and Robot Learning</span>
    <span class="links">
        <a target="_blank" href="https://clip-rt.github.io">Project Page</a>
        <a target="_blank" href="https://arxiv.org/abs/2411.00508">Paper</a>
        <a target="_blank" href="https://github.com/clip-rt/clip-rt">Code</a>
    </span>
</p>
<video playsinline autoplay muted loop style="width: 100%" class="webby">
    <source src="/img/cliprt_overview.mp4" type="video/mp4"></source>
</video>
<hr>


<a name="prograsp"></a>
<h2 class="pubt">PROGrasp: Pragmatic Human-Robot Communication for Object Grasping</h2>
<p class="pubd">
    <span class="authors"><span class="u">Gi-Cheon Kang</span>, Junghyun Kim, Jaein Kim, Byoung-Tak Zhang</span><br>
    <span class="conf">ICRA 2024 (Oral)</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2309.07759">Paper</a>
        <a target="_blank" href="https://github.com/gicheonkang/prograsp">Code</a>
    </span>
</p>
<video playsinline autoplay muted loop style="width: 100%" class="webby">
    <source src="/img/prograsp_overview.mp4" type="video/mp4"></source>
</video>
<hr>

<a name="/gst"></a>
<h2 class="pubt">The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training</h2>
<p class="pubd">
    <span class="authors"><span class="u">Gi-Cheon Kang</span>, Sungdong Kim<sup>*</sup>, Jin-Hwa Kim<sup>*</sup>, Donghyun Kwak<sup>*</sup>, Byoung-Tak Zhang</span><br>
    <span class="conf">CVPR 2023</span><br>
    <span class="conf">ICML 2022 Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward</span>
    <span class="links">
        <a target="_blank" href="https://gicheonkang.com/projects/gst/">Project Page</a>
        <a target="_blank" href="https://arxiv.org/abs/2205.12502">Paper</a>
        <a target="_blank" href="https://github.com/gicheonkang/gst-visdial">Code</a>
        <a target="_blank" href="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/gicheonkang/gicheonkang.github.io/master/docs/GST-23-slide.pdf">Slides</a>
        <a target="_blank" href="https://youtu.be/SrOzZRyqezs">Video</a>
    </span>
</p>
<img src="/img/gst_overview.gif">
<hr>




<a name="/talks"></a>

# Invited Talks

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;">PROGrasp: Pragmatic Human-Robot Communication for Object Grasping</h2>
        <p class="talkd">
            IEEE International Conference on Robotics and Automation (ICRA, May 2024)
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;">The Evolution of Language Models: From Basic NLP to ChatGPT and Beyond</h2>
        <p class="talkd">
            Dept. of Energy Resources Engineering, Seoul National University (Nov. 2023)
        </p>
    </div>
</div>
         
<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;">The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training</h2>
        <p class="talkd">
            IEEE RO-MAN Workshop on Learning by Asking for Intelligent Robots and Agents (Aug. 2023)
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;">Reasoning Visual Dialog with Sparse Graph Learning and Knowledge Transfer</h2>
        <p class="talkd">
            KSC 2021 - Top-tier Conference Paper Presentation Session (Dec. 2021) <br>
            Annual Conference on Human and Cognitive Language Technology (Oct. 2021)
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;">Dual Attention Networks for Visual Reference Resolution in Visual Dialog</h2>
        <p class="talkd">
            ICCV 2019 - Video Turing Test Workshop (Spotlight Talk) (Nov. 2019) <br>
            SK Telecom AI Center (Sep. 2019)
        </p>
    </div>
</div>
<hr>


<a name="/services"></a>

# Services

<div class="row">
    <div class="col-xs-12">
        <div class="talkt">
            <b>Reviewing</b><br>
            &#128997; = ML or AI / &#129001; = NLP / &#128998; = Robotics / &#129000; = Workshops
        </div>
        <div class="talkt">
            &#128997; Neural Information Processing Systems (NeurIPS), 2023-2024
        </div>
        <div class="talkt">
            &#128997; International Conference on Machine Learning (ICML), 2024-2025
        </div>
        <div class="talkt">
            &#128997; International Conference on Learning Representations (ICLR), 2024-2025
        </div>
        <div class="talkt">
            &#128997; AAAI Conference on Artificial Intelligence (AAAI), 2025
        </div>
        <div class="talkt">
            &#129001; ACL Rolling Review (ARR), 2024
        </div>
        <div class="talkt">
            &#129001; Annual Meeting of the Association for Computational Linguistics (ACL), 2023-2024
        </div>
        <div class="talkt">
            &#129001; Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022-2023
        </div>
        <div class="talkt">
            &#128998; IEEE Robotics and Automation Letters (RA-L), 2024
        </div>
        <div class="talkt">
            &#128998; IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2024
        </div>
        <div class="talkt">
            &#129000; CoRL Workshop on Language and Robot Learning (LangRob), 2024
        </div>
        <div class="talkt">
            &#129000; ICLR Workshop on Reliable and Responsible Foundation Models, 2024
        </div>
        <div class="talkt">
            &#129000; ICML Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward, 2022
        </div>
    </div>
</div>
<hr>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td>
        <br>
        <p align="right"><font size="2">
          <a href="https://abhishekdas.com/">(Courtesy: Abhishek Das)</a>
          </font>
        </p>
      </td>
    </tr>
</table>

<script src="/js/jquery.min.js"></script>
<script type="text/javascript">
    $('ul:gt(0) li:gt(12)').hide();
    $('#read-more-button > a').click(function() {
        $('ul:gt(0) li:gt(12)').show();
        $('#read-more-button').hide();
    });
</script>

---

[1]: http://en.snu.ac.kr
[2]: https://aiis.snu.ac.kr/eng/
[3]: https://scholar.google.com/citations?user=sYTUOu8AAAAJ&hl=en
[4]: https://gicheonkang.com
[5]: http://www.ajou.ac.kr/en/
[embodied_ai]: https://embodied-ai.org
[multimodal_ai]: https://arxiv.org/abs/2309.10020
[gvcci]: https://arxiv.org/abs/2307.05963
[prograsp]: https://arxiv.org/abs/2309.07759
[gst]: https://arxiv.org/abs/2205.12502
[sglkt]: https://arxiv.org/abs/2004.06698
[masn]: https://aclanthology.org/2021.acl-long.481
[dan]: https://www.aclweb.org/anthology/D19-1209
[lpart]: https://arxiv.org/abs/2005.02137
[c3]: https://ctrlgenworkshop.github.io/camready/40/CameraReady/NIPS_Workshop_camera_ready.pdf
[sfa]: https://openaccess.thecvf.com/content/CVPR2022W/HCIS/papers/Lee_Improving_Robustness_to_Texture_Bias_via_Shape-Focused_Augmentation_CVPRW_2022_paper.pdf
[youlchon]: https://eng.nongshim.com/sustainability/people/youlchon
[ipai]: https://gsai.snu.ac.kr/en/
[cliprt]: https://clip-rt.github.io
[pga]: https://arxiv.org/abs/2310.12547
[socraticplanner]: https://arxiv.org/abs/2404.15190
