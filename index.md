---
layout:     page
title:
permalink:  /
---

<div class="row">
    <div class="col-sm-6 col-xs-12">
        <img src="/img/cover2.jpg" class="profile">
    </div>
    <div class="col-sm-6 col-xs-12" style="margin-bottom: 0;">
        Ph.D. Student<br>
        Graduate School of AI<br>
        Seoul National University<br>
        <a target="_blank" href="mailto:gchnkang@gmail.com">gchnkang at gmail dot com</a>
    </div>
</div>
<hr>


<a name="/bio"></a>

# Bio

I am a fourth year Ph.D. student in the Graduate School of AI at [Seoul National University][1], advised by [Prof. Byoung-Tak Zhang][3]. My research straddles machine learning, natural language processing, computer vision, and robotics. I'm focused on connecting language with perception and action, enabling machines to understand the semantics of the physical world. The long-term goal of my research is to build machines that can
- perceive their everyday surroundings through vision or other senses,
- communicate with humans via grounded natural language,
- make reliable decisions using language, vision, etc.    

Specific topics include: (i) robots performing the desired task through language interaction ([arXiv'23][prograsp], [IROS'23][gvcci]), (ii) visually-grounded dialog agents ([CVPR'23][gst], [EMNLP'21][sglkt], [ACL'21][masn], [EMNLP'19][dan]), and (iii) other topics like robustness and image generation ([CVPRW'22][sfa], [NeurIPSW'21][c3], [ICASSP'20][lpart]).

I was fortunate to collaborate with researchers in NAVER and SK Telecom.

Prior to joining Ph.D. program, I did my master study in Cognitive Science at [Seoul National University][1]. Studying cognitive science has had a great influence on my research. I earned my Bachelor's degree in Computer Science from [Ajou University][5].


<p style='text-align:center; color:#9A2617; font-weight:bold'>I'm actively looking for Ph.D. internship positions in 2024! Please let me know if you have any opportunities for me.</p>

---

<a name="/news"></a>

# News

- [Nov 2023] I'll give a talk at Dept. of Energy Resources Engineering at Seoul National University (Title: "The Evolution of Language Models:
From Basic NLP to ChatGPT and Beyond").
- [Oct 2023] Two preprints (<a href="https://arxiv.org/abs/2309.07759">PROGrasp</a> and <a href="https://arxiv.org/abs/2310.12547">PGA</a>) are released! 
- [Jun 2023] One paper is accepted to <a href="https://ieee-iros.org">IROS 2023</a>!
- [Mar 2023] Happy to announce that <a href="https://arxiv.org/abs/2205.12502">our paper</a> is accepted to <a href="https://cvpr2023.thecvf.com">CVPR 2023</a>!
- [Jun 2022] One paper is accepted to ICML 2022 <a href="[https://sites.google.com/nycu.edu.tw/hcis/home](https://pretraining.github.io)">Pre-training Workshop</a>.
- [May 2022] Thrilled to announce that our <a href="https://arxiv.org/abs/2205.12502">new preprint</a> is released!
- [Apr 2022] One paper is accepted to CVPR 2022 <a href="https://sites.google.com/nycu.edu.tw/hcis/home">HCIS Workshop</a>.
- [Dec 2021] I gave an invited talk at Korea Software Congress.
- [Oct 2021] One paper is accepted to NeurIPS 2021 CtrlGen Workshop.
- [Aug 2021] One paper is accepted to Findings of EMNLP 2021.
- [May 2021] One paper is accepted to ACL 2021.
- [Sep 2020] I'm starting my Ph.D. in this fall.
- [Jun 2020] From July, I'll join <a href="https://aiis.snu.ac.kr">SNU AI Institute</a> (AIIS) as a researcher.
- [Jan 2020] Our paper has been accepted to ICASSP 2020!
- [Dec 2019] From January, I'll be a research intern at <a href="https://www.skt.ai">SK T-Brain</a>!
- [Nov 2019] I gave a spotlight talk at <a href="https://videoturingtest.github.io">Video Turing Test workshop</a>, ICCV 2019.
- [Oct 2019] I gave an invited talk at <a href="https://www.skt.ai">SK Telecom AI Center</a>.
- [Aug 2019] Excited to announce that <a href="https://arxiv.org/abs/1902.09368">our paper</a> has been accepted to <a href="https://www.emnlp-ijcnlp2019.org/">EMNLP 2019</a>.
- [Jun 2019] Our proposed method ranks <b>3rd place</b> in <a href="https://visualdialog.org/challenge/2019">Visual Dialog Challenge 2019</a>!!
- [Aug 2018] We have a paper accepted to ECCV 2018 Workshop on <a href="http://vizwiz.org/workshop/">VizWiz Grand Challenge</a>.

<div id="read-more-button">
    <a nohref>Read more</a>
</div>

<hr>


<a name="/publications"></a>

# Publications

<a name="/pga"></a>
<h2 class="pubt">PGA: Personalizing Grasping Agents with Single Human-Robot Interaction</h2>
<p class="pubd">
    <span class="authors">Junghyun Kim, <span class="u">Gi-Cheon Kang</span><sup>*</sup>, Jaein Kim<sup>*</sup>, Seoyun Yang, Minjoon Jung, Byoung-Tak Zhang</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2310.12547">Paper</a>
    </span>
</p>
<img src="/img/pga_overview.png">
<hr>

<a name="prograsp"></a>
<h2 class="pubt">PROGrasp: Pragmatic Human-Robot Communication for Object Grasping</h2>
<p class="pubd">
    <span class="authors"><span class="u">Gi-Cheon Kang</span>, Junghyun Kim, Jaein Kim, Byoung-Tak Zhang</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2309.07759">Paper</a>
    </span>
</p>
<video playsinline autoplay muted loop style="width: 100%" class="webby">
    <source src="/img/prograsp_overview.mp4" type="video/mp4"></source>
</video>
<hr>

<a name="/gst"></a>
<h2 class="pubt">The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training</h2>
<p class="pubd">
    <span class="authors"><span class="u">Gi-Cheon Kang</span>, Sungdong Kim<sup>*</sup>, Jin-Hwa Kim<sup>*</sup>, Donghyun Kwak<sup>*</sup>, Byoung-Tak Zhang</span><br>
    <span class="conf">CVPR 2023</span><br>
    <span class="conf">ICML 2022 Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward</span>
    <span class="links">
        <a target="_blank" href="https://gicheonkang.com/projects/gst/">Project Page</a>
        <a target="_blank" href="https://arxiv.org/abs/2205.12502">Paper</a>
        <a target="_blank" href="https://github.com/gicheonkang/gst-visdial">Code</a>
        <a target="_blank" href="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/gicheonkang/gicheonkang.github.io/master/docs/GST-23-slide.pdf">Slides</a>
        <a target="_blank" href="https://youtu.be/SrOzZRyqezs">Video</a>
    </span>
</p>
<img src="/img/gst_overview.gif">
<hr>

<a name="/gvcci"></a>
<h2 class="pubt">GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic Manipulation</h2>
<p class="pubd">
    <span class="authors">Junghyun Kim, <span class="u">Gi-Cheon Kang</span><sup>*</sup>, Jaein Kim<sup>*</sup>, Suyeon Shin, Byoung-Tak Zhang</span><br>
    <span class="conf">IROS 2023</span><br>
    <span class="conf">Oral Presentation</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2307.05963">Paper</a>
        <a target="_blank" href="https://github.com/JHKim-snu/GVCCI">Code</a>
    </span>
</p>
<img src="/img/gvcci_overview.png">
<hr>

<a name="/sfa"></a>
<h2 class="pubt">Improving Robustness to Texture Bias via Shape-focused Augmentation</h2>
<p class="pubd">
    <span class="authors">Sangjun Lee, Inwoo Hwang, <span class="u">Gi-Cheon Kang</span>, Byoung-Tak Zhang</span><br>
    <span class="conf">CVPR 2022 Workshop on Human-centered Intelligent Services: Safety and Trustworthy</span>
    <span class="links">
        <a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2022W/HCIS/papers/Lee_Improving_Robustness_to_Texture_Bias_via_Shape-Focused_Augmentation_CVPRW_2022_paper.pdf">Paper</a>
    </span>
</p>
<img src="/img/sfa_overview.png">
<hr>

<a name="/sglkt"></a>
<h2 class="pubt">Reasoning Visual Dialog with Sparse Graph Learning and Knowledge Transfer</h2>
<p class="pubd">
    <span class="authors"><span class="u">Gi-Cheon Kang</span>, Junseok Park, Hwaran Lee, Byoung-Tak Zhang<sup>*</sup>, Jin-Hwa Kim<sup>*</sup></span><br>
    <span class="conf">EMNLP 2021 Findings</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2004.06698">Paper</a>
        <a target="_blank" href="https://github.com/gicheonkang/sglkt-visdial">Code</a>
        <a target="_blank" href="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/gicheonkang/gicheonkang.github.io/master/docs/SGLKT-21-slide.pdf">Slides</a>
    </span>
</p>
<img src="/img/sglkt_overview.png">
<hr>

<a name="/masn"></a>
<h2 class="pubt">Attend What You Need: Motion-Appearance Synergistic Networks for Video Question Answering</h2>
<p class="pubd">
    <span class="authors">Ahjeong Seo, <span class="u">Gi-Cheon Kang</span>, Joonhan Park, Byoung-Tak Zhang</span><br>
    <span class="conf">ACL 2021</span>
    <span class="links">
        <a target="_blank" href="https://aclanthology.org/2021.acl-long.481">Paper</a>
        <a target="_blank" href="https://github.com/ahjeongseo/MASN-pytorch">Code</a>
    </span>
</p>
<img src="/img/masn_overview.png">
<hr>

<a name="/c3"></a>
<h2 class="pubt">C<sup>3</sup>: Contrastive Learning for Cross-domain Correspondence in Few-shot Image Generation</h2>
<p class="pubd">
    <span class="authors">Hyukgi Lee, <span class="u">Gi-Cheon Kang</span>, Chang-Hoon Jeong, Hanwool Sul, Byoung-Tak Zhang</span><br>
    <span class="conf">NeurIPS 2021 Workshop on Controllable Generative Modeling in Language and Vision</span>
    <span class="links">
        <a target="_blank" href="https://ctrlgenworkshop.github.io/camready/40/CameraReady/NIPS_Workshop_camera_ready.pdf">Paper</a>
    </span>
</p>
<img src="/img/c3_overview.png">
<hr>

<a name="/lpart"></a>
<h2 class="pubt">Label Propagation Adaptive Resonance Theory for Semi-Supervised Continuous Learning</h2>
<p class="pubd">
    <span class="authors">Taehyeong Kim, Injune Hwang, <span class="u">Gi-Cheon Kang</span>, Won-Seok Choi, Hyunseo Kim, Byoung-Tak Zhang</span><br>
    <span class="conf">ICASSP 2020</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2005.02137">Paper</a>
    </span>
</p>
<img src="/img/lpart_overview.png">
<hr>

<a name="/dan"></a>
<h2 class="pubt">Dual Attention Networks for Visual Reference Resolution in Visual Dialog</h2>
<p class="pubd">
    <span class="authors"><span class="u">Gi-Cheon Kang</span>, Jaeseo Lim, Byoung-Tak Zhang</span><br>
    <span class="conf">EMNLP 2019</span><br>
    <span class="conf">3rd Place in VisDial Challenge @ CVPR 2019</span>
    <span class="links">
        <a target="_blank" href="https://www.aclweb.org/anthology/D19-1209">Paper</a>
        <a target="_blank" href="https://github.com/gicheonkang/DAN-VisDial">Code</a>
        <a target="_blank" href="https://docs.google.com/viewer?url=https://raw.githubusercontent.com/gicheonkang/gicheonkang.github.io/master/files/DAN-19-slide.pdf">Slides</a>        
    </span>
</p>
<img src="/img/dan_overview.jpg">
<hr>


<a name="/talks"></a>

# Invited Talks

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;">The Evolution of Language Models: From Basic NLP to ChatGPT and Beyond</h2>
        <p class="talkd">
            Dept. of Energy Resources Engineering, Seoul National University (Nov. 2023)
        </p>
    </div>
</div>
         
<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;">The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training</h2>
        <p class="talkd">
            IEEE RO-MAN Workshop on Learning by Asking for Intelligent Robots and Agents (Aug. 2023)
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;">Reasoning Visual Dialog with Sparse Graph Learning and Knowledge Transfer</h2>
        <p class="talkd">
            KSC 2021 - Top-tier Conference Paper Presentation Session (Dec. 2021) <br>
            Annual Conference on Human and Cognitive Language Technology (Oct. 2021)
        </p>
    </div>
</div>

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;">Dual Attention Networks for Visual Reference Resolution in Visual Dialog</h2>
        <p class="talkd">
            ICCV 2019 - Video Turing Test Workshop (Spotlight Talk) (Nov. 2019) <br>
            SK Telecom AI Center (Sep. 2019)
        </p>
    </div>
</div>
<hr>


<a name="/services"></a>

# Services

<div class="row">
    <div class="col-xs-12">
        <div class="talkt">
            <b>Reviewing</b>
        </div>
        <div class="talkt">
            International Conference on Learning Representations (ICLR), 2024
        </div>
        <div class="talkt">
            Neural Information Processing Systems (NeurIPS), 2023
        </div>
        <div class="talkt">
            Annual Meeting of the Association for Computational Linguistics (ACL), 2023
        </div>
        <div class="talkt">
            Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022-2023
        </div>
    </div>
</div>
<hr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td>
        <br>
        <p align="right"><font size="2">
          <a href="https://abhishekdas.com/">(Courtesy: Abhishek Das)</a>
          <!-- <a href="http://www.cs.berkeley.edu/~barron/"> this website</a> -->
          </font>
        </p>
      </td>
    </tr>
</table>

<script src="/js/jquery.min.js"></script>
<script type="text/javascript">
    $('ul:gt(0) li:gt(12)').hide();
    $('#read-more-button > a').click(function() {
        $('ul:gt(0) li:gt(12)').show();
        $('#read-more-button').hide();
    });
</script>

---

[1]: http://en.snu.ac.kr
[2]: https://aiis.snu.ac.kr/eng/
[3]: https://bi.snu.ac.kr/~btzhang/
[4]: https://gicheonkang.com
[5]: http://www.ajou.ac.kr/en/
[gvcci]: https://arxiv.org/abs/2307.05963
[prograsp]: https://arxiv.org/abs/2309.07759
[gst]: https://arxiv.org/abs/2205.12502
[sglkt]: https://arxiv.org/abs/2004.06698
[masn]: https://aclanthology.org/2021.acl-long.481
[dan]: https://www.aclweb.org/anthology/D19-1209
[lpart]: https://arxiv.org/abs/2005.02137
[c3]: https://ctrlgenworkshop.github.io/camready/40/CameraReady/NIPS_Workshop_camera_ready.pdf
[sfa]: https://openaccess.thecvf.com/content/CVPR2022W/HCIS/papers/Lee_Improving_Robustness_to_Texture_Bias_via_Shape-Focused_Augmentation_CVPRW_2022_paper.pdf 
