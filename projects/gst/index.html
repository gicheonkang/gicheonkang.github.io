
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>GST</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="/img/gst_overview.gif">
    <meta property="og:image:type" content="image/gif">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://gicheonkang.com/projects/gst/"/>
    <meta property="og:title" content="The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training" />
    <meta property="og:description" content="Visual dialog (VisDial) is a task of answering a sequence of questions grounded in an image, using the dialog history as context. Prior work has trained the dialog agents solely on VisDial data via supervised learning or leveraged pre- training on related vision-and-language datasets. This paper presents a semi-supervised learning approach for visually- grounded dialog, called Generative Self-Training (GST), to leverage unlabeled images on the Web. Specifically, GST first retrieves in-domain images through out-of-distribution detection and generates synthetic dialogs regarding the im- ages via multimodal conditional text generation. GST then trains a dialog agent on the synthetic and the original Vis- Dial data. As a result, GST scales the amount of training data up to an order of magnitude that of VisDial (1.2M → 12.9M QA data). For robust training of the synthetic di- alogs, we also propose perplexity-based data selection and multimodal consistency regularization. Evaluation on Vis- Dial v1.0 and v0.9 datasets shows that GST achieves new state-of-the-art results on both datasets. We further observe the robustness of GST against both visual and textual ad- versarial attacks. Finally, GST yields strong performance gains in the low-data regime."
 />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training" />
    <meta name="twitter:description" content="Visual dialog (VisDial) is a task of answering a sequence of questions grounded in an image, using the dialog history as context. Prior work has trained the dialog agents solely on VisDial data via supervised learning or leveraged pre- training on related vision-and-language datasets. This paper presents a semi-supervised learning approach for visually- grounded dialog, called Generative Self-Training (GST), to leverage unlabeled images on the Web. Specifically, GST first retrieves in-domain images through out-of-distribution detection and generates synthetic dialogs regarding the im- ages via multimodal conditional text generation. GST then trains a dialog agent on the synthetic and the original Vis- Dial data. As a result, GST scales the amount of training data up to an order of magnitude that of VisDial (1.2M → 12.9M QA data). For robust training of the synthetic di- alogs, we also propose perplexity-based data selection and multimodal consistency regularization. Evaluation on Vis- Dial v1.0 and v0.9 datasets shows that GST achieves new state-of-the-art results on both datasets. We further observe the robustness of GST against both visual and textual ad- versarial attacks. Finally, GST yields strong performance gains in the low-data regime." />
    <meta name="twitter:image" content="/img/gst_overview.gif" />


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚡</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>The Dialog Must Go On</b>:</br>Improving Visual Dialog via Generative Self-Training</br> 
                <small>
                CVPR 2023
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://gicheonkang.com">
                          Gi-Cheon Kang<sup>1</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=xKrSnDoAAAAJ&hl=en">
                            Sungdong Kim<sup>2*</sup>
                        </a>
                    </li>
                    <li>
                        <a href="http://wityworks.com">
                          Jin-Hwa Kim<sup>2*</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?hl=en&user=MROzd8gAAAAJ">
                          Donghyun Kwak<sup>2*</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://bi.snu.ac.kr/~btzhang/">
                          Byoung-Tak Zhang<sup>1</sup>
                        </a>
                    </li>
                    </br>
                    <li><sup>1</sup>Seoul National University</li>
                    <li><sup>2</sup>NAVER</li> 
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2205.12502">
                            <image src="img/paper_front.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="text-center">
	    <img src="/img/gst_overview.gif" width="70%">
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
Visual dialog (VisDial) is a task of answering a sequence of questions grounded in an image, using the dialog history as context. Prior work has trained the dialog agents solely on VisDial data via supervised learning or leveraged pre- training on related vision-and-language datasets. This paper presents a semi-supervised learning approach for visually- grounded dialog, called Generative Self-Training (GST), to leverage unlabeled images on the Web. Specifically, GST first retrieves in-domain images through out-of-distribution detection and generates synthetic dialogs regarding the im- ages via multimodal conditional text generation. GST then trains a dialog agent on the synthetic and the original Vis- Dial data. As a result, GST scales the amount of training data up to an order of magnitude that of VisDial (1.2M → 12.9M QA data). For robust training of the synthetic di- alogs, we also propose perplexity-based data selection and multimodal consistency regularization. Evaluation on Vis- Dial v1.0 and v0.9 datasets shows that GST achieves new state-of-the-art results on both datasets. We further observe the robustness of GST against both visual and textual ad- versarial attacks. Finally, GST yields strong performance gains in the low-data regime.
                </p>
            </div>
        </div>
<br>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <textarea id="bibtex" class="form-control" readonly>
@inproceedings{kang2023dialog,
    title={The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training},
    author={Kang, Gi-Cheon and Kim, Sungdong and Kim, Jin-Hwa and Kwak, Donghyun and Zhang, Byoung-Tak},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2023},
    pages={6746-6756}
}</textarea>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                This work was supported by the SNU-NAVER Hyperscale AI Center and the Institute of Information & Communications Technology Planning & Evaluation (IITP) (2021-0-01343-GSAI/40%, 2022-0- 00953-PICA/30%, 2022-0-00951-LBA/20%, 2021-0-02068- AIHub/10%) grant funded by the Korean government.
                <br><br>
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and <a href="https://dorverbin.github.io/refnerf">Ref-NeRF</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
