
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>GST</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="/img/gst_overview.gif">
    <meta property="og:image:type" content="image/gif">
    <meta property="og:image:width" content="1296">
    <meta property="og:image:height" content="840">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://gicheonkang.com/projects/gst/"/>
    <meta property="og:title" content="The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training" />
    <meta property="og:description" content="Visual dialog (VisDial) is a task of answering a sequence of questions grounded in an image, using the dialog history as context. Prior work has trained the dialog agents solely on VisDial data via supervised learning or leveraged pre-training on related vision-and-language datasets. This paper presents a semi-supervised learning approach for visually-grounded dialog, called Generative Self-Training (GST), to leverage unlabeled images on the Web. Specifically, GST first retrieves in-domain images through out-of-distribution detection and generates synthetic dialogs regarding the images via multimodal conditional text generation. GST then trains a dialog agent on the synthetic and the original VisDial data. As a result, GST scales the amount of training data up to an order of magnitude that of VisDial (1.2M → 12.9M QA data). For robust training of the synthetic dialogs, we also propose perplexity-based data selection and multimodal consistency regularization. Evaluation on VisDial v1.0 and v0.9 datasets shows that GST achieves new state-of-the-art results on both datasets. We further observe the robustness of GST against both visual and textual adversarial attacks. Finally, GST yields strong performance gains in the low-data regime."
 />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training" />
    <meta name="twitter:description" content="Visual dialog (VisDial) is a task of answering a sequence of questions grounded in an image, using the dialog history as context. Prior work has trained the dialog agents solely on VisDial data via supervised learning or leveraged pre- training on related vision-and-language datasets. This paper presents a semi-supervised learning approach for visually- grounded dialog, called Generative Self-Training (GST), to leverage unlabeled images on the Web. Specifically, GST first retrieves in-domain images through out-of-distribution detection and generates synthetic dialogs regarding the im- ages via multimodal conditional text generation. GST then trains a dialog agent on the synthetic and the original Vis- Dial data. As a result, GST scales the amount of training data up to an order of magnitude that of VisDial (1.2M → 12.9M QA data). For robust training of the synthetic di- alogs, we also propose perplexity-based data selection and multimodal consistency regularization. Evaluation on Vis- Dial v1.0 and v0.9 datasets shows that GST achieves new state-of-the-art results on both datasets. We further observe the robustness of GST against both visual and textual ad- versarial attacks. Finally, GST yields strong performance gains in the low-data regime." />
    <meta name="twitter:image" content="/img/gst_overview.gif" />


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚡</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script src="js/video_comparison.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>The Dialog Must Go On</b>:</br>Improving Visual Dialog via Generative Self-Training</br> 
                <small>
                CVPR 2023
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://gicheonkang.com">
                          Gi-Cheon Kang<sup>1</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=xKrSnDoAAAAJ&hl=en">
                            Sungdong Kim<sup>2*</sup>
                        </a>
                    </li>
                    <li>
                        <a href="http://wityworks.com">
                          Jin-Hwa Kim<sup>2*</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?hl=en&user=MROzd8gAAAAJ">
                          Donghyun Kwak<sup>2*</sup>
                        </a>
                    </li>
                    <li>
                        <a href="https://bi.snu.ac.kr/~btzhang/">
                          Byoung-Tak Zhang<sup>1</sup>
                        </a>
                    </li>
                    </br>
                    <li><sup>1</sup>Seoul National University</li>
                    <li><sup>2</sup>NAVER</li> 
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2205.12502">
                            <image src="img/paper_front.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/gicheonkang/gst-visdial">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtu.be/SrOzZRyqezs">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <image src="/img/gst_overview.gif" class="img-responsive" alt="overview">
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
Visual dialog (VisDial) is a task of answering a sequence of questions grounded in an image, using the dialog history as context. Prior work has trained the dialog agents solely on VisDial data via supervised learning or leveraged pre-training on related vision-and-language datasets. This paper presents a semi-supervised learning approach for visually-grounded dialog, called Generative Self-Training (GST), to leverage unlabeled images on the Web. Specifically, GST first retrieves in-domain images through out-of-distribution detection and generates synthetic dialogs regarding the images via multimodal conditional text generation. GST then trains a dialog agent on the synthetic and the original VisDial data. As a result, GST scales the amount of training data up to an order of magnitude that of VisDial (1.2M → 12.9M QA data). For robust training of the synthetic dialogs, we also propose perplexity-based data selection and multimodal consistency regularization. Evaluation on VisDial v1.0 and v0.9 datasets shows that GST achieves new state-of-the-art results on both datasets. We further observe the robustness of GST against both visual and textual adversarial attacks. Finally, GST yields strong performance gains in the low-data regime.
                </p>
            </div>
        </div>
<br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://youtu.be/SrOzZRyqezs" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>
<br>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Visualization of the synthetic data
                </h3>
                <p class="text-justify"> The synthetic visual dialog data generated by our proposed models is shown below. The red-colored text denotes incorrect answers.
                </p>
                <image src="img/dialog.png" class="img-responsive"> 
            </div>
        </div>
<br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Experiments
                </h3>
                <p class="text-justify"> The student model trained on the synthetic visual dialog data achieves a new state-of-the-art performance on VisDial v0.9 and v1.0 datasets. Furthermore, the student model significantly improves the robustness against adversarial attacks compared with the teacher model. The teacher model is our baseline model which is not trained on the synthetic data.
                </p>
                <image src="img/sota.png" class="img-responsive">
                <image src="img/adv.png" class="img-responsive">
            </div>
        </div>


<br>        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <textarea id="bibtex" class="form-control" readonly>
@inproceedings{kang2023dialog,
    title={The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training},
    author={Kang, Gi-Cheon and Kim, Sungdong and Kim, Jin-Hwa and Kwak, Donghyun and Zhang, Byoung-Tak},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2023},
    pages={6746-6756}
}</textarea>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                This work was supported by the SNU-NAVER Hyperscale AI Center and the Institute of Information & Communications Technology Planning & Evaluation (IITP) (2021-0-01343-GSAI/40%, 2022-0- 00953-PICA/30%, 2022-0-00951-LBA/20%, 2021-0-02068- AIHub/10%) grant funded by the Korean government.
                <br><br>
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and <a href="https://dorverbin.github.io/refnerf">Ref-NeRF</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
